{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from lpci import LPCI, EvaluateLPCI\n",
    "from panelsplit import PanelSplit\n",
    "from sklearn_quantile import RandomForestQuantileRegressor\n",
    "import multiprocessing as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a guide for implementing the LPCI algorithm for a regression forecasting model.\n",
    "\n",
    "We will refrain from a mathematical description in this notebook and focus on the code. Please see the README for a more technical overview.\n",
    "\n",
    "We assume that point predictions have been obtained on a calibration and test set. Our implementation then undertakes the following steps:\n",
    "\n",
    "1) Compute non-conformity scores (residuals) on calibration set.\n",
    "2) Train a Quantile Random Forest (QRF) where X = lagged residuals and y = current calibration residual.\n",
    "3) Use the fitted QRF to construct prediction interval with conditional quantiles.\n",
    "4) Evaluate and plot.\n",
    "\n",
    "Let's start with what is required to instantiate the class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some dummy calibration and test predictions for the purposes of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the panel data\n",
    "\n",
    "#most importantly, you must account for data update delay. This should directly correspond to your time col.\n",
    "#In our case, the data is yearly, and released with a 1 year delay.\n",
    "eval_delay = 1\n",
    "\n",
    "#set the units\n",
    "unit_col = 'isocode'\n",
    "units = ['DEU', 'ESP', 'GBR', 'FRA', 'ITA']\n",
    "\n",
    "#now the time\n",
    "time_col = 'year'\n",
    "id_vars = [unit_col, time_col]\n",
    "calibration_years = np.arange(2015, 2020, dtype = int) #recall that this is end-exclusive\n",
    "test_years = np.arange(2020, 2024, dtype = int) #recall that this is end-exclusive\n",
    "\n",
    "#construct empty panels\n",
    "cal_set = pd.DataFrame()\n",
    "cal_set[id_vars] = [[unit, year] for unit, year in itertools.product(units, calibration_years)]\n",
    "test_set = pd.DataFrame()\n",
    "test_set[id_vars] = [[unit, year] for unit, year in itertools.product(units, test_years)]\n",
    "\n",
    "#format year column as an integer\n",
    "cal_set[time_col] = cal_set[time_col].astype(int)\n",
    "test_set[time_col] = test_set[time_col].astype(int)\n",
    "cal_set[unit_col] = cal_set[unit_col].astype('category')\n",
    "test_set[unit_col] = test_set[unit_col].astype('category')\n",
    "\n",
    "#generate random predictions (only positive values)\n",
    "preds_col = 'preds'\n",
    "true_col = 'true'\n",
    "\n",
    "#absolute because we just want positive values for our dummy data\n",
    "cal_set[preds_col] = np.abs(np.random.normal(0, 10_000, cal_set.shape[0]))\n",
    "cal_set[true_col] = np.abs(np.random.normal(0, 10_000, cal_set.shape[0]))\n",
    "\n",
    "test_set[preds_col] = np.abs(np.random.normal(0, 10_000, test_set.shape[0]))\n",
    "test_set[true_col] = np.abs(np.random.normal(0, 10_000, test_set.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to instantiate an instance of LPCI. Feel free to play around and check out the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpci = LPCI(\n",
    "    eval_delay,\n",
    "    cal_set,\n",
    "    test_set, \n",
    "    unit_col=unit_col, \n",
    "    time_col=time_col, \n",
    "    preds_col=preds_col, \n",
    "    true_col=true_col\n",
    "    )\n",
    "\n",
    "#for example, note that we construct a \"full dataframe\" by concatenating the calibration and test sets.\n",
    "#This is the .df attribute of the class\n",
    "\n",
    "display(lpci.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute non-conformity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current implementation assumes that the non-conformity score is equivalent to the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpci.nonconformity_score(lpci.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train a Quantile Random Forest on the calibration set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Generate panel of lagged residuals and group identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through each of the steps that generate our panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to generate the residuals\n",
    "df = lpci.nonconformity_score(lpci.df)\n",
    "print(\"Below you see the residuals of the model on the calibration set.\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we need to generate the lagged residuals\n",
    "window_size = 1\n",
    "\n",
    "df_lag = lpci.lag(df, 'residuals', np.arange(1, window_size+1))\n",
    "print(f\"Below you see the residuals of the model on the calibration set with lagged residuals up to {window_size} periods.\")\n",
    "display(df_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the importance of the update delay parameter. For example in our dummy data, ('DEU', 2015) represents our prediction made in 2015 for the outcome in 2016. But we would not be able to evaluate this error until 2017. The eval_delay parameter ensures this is taken into account when generating lagged residuals. We can only include the residual for 2015 as a lagged_residual feature in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one can also utilise exponential smoothing on the lagged residuals and/or filling of NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the smoothed residuals\n",
    "window_size = 1\n",
    "\n",
    "df_lag_smooth = lpci.lag(df, 'residuals', np.arange(1, window_size+1), decay = 0.8, adjust = False)\n",
    "print(f\"Below you see the residuals of the model on the calibration set with lagged residuals up to {window_size} periods.\")\n",
    "display(df_lag_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the smoothed residuals and fill na with 0\n",
    "window_size = 1\n",
    "\n",
    "df_lag_smooth_fillna = lpci.lag(df, 'residuals', np.arange(1, window_size+1), decay = 0.8, adjust = False, fillna = 0)\n",
    "print(f\"Below you see the residuals of the model on the calibration set with lagged residuals up to {window_size} periods.\")\n",
    "display(df_lag_smooth_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with simple lags (i.e. no smoothing), a window size of 2 and without filling NAs for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "df = lpci.lag(df, 'residuals', np.arange(1, window_size+1))\n",
    "print(f\"Below you see the residuals of the model on the calibration set with lagged residuals up to {window_size} periods.\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop observations where there are NAs in the lagged residuals\n",
    "df = df.dropna(subset = [x for x in df.columns if 'lag' in x], axis = 0, how = 'any')\n",
    "print(f\"Below you see the residuals of the model on the calibration set with lagged residuals up to {window_size} periods after dropping rows with NAs in the lagged residuals.\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we add group identifiers\n",
    "cat_method = {'isocode': 'one_hot_encode'}\n",
    "df = lpci.cat_engineer(df, cat_method)\n",
    "print(f\"Below you now see the panel required to train our QRF!\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the complete function\n",
    "window_size = 2\n",
    "cat_method = {'isocode': 'one_hot_encode'}\n",
    "\n",
    "df, features, target_col = lpci.prepare_df(\n",
    "    window_size,\n",
    "    cat_method = cat_method,\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the warning generated here, we will explore this further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Tune QRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to tune our Quantile Random Forest. But, we must decide on a set of key parameters.\n",
    "\n",
    "First let's consider possible cross validation strategies:\n",
    "\n",
    "1) Time series cross validation using panel split\n",
    "2) Standard k-fold cross validation\n",
    "\n",
    "In cases where you have a small calibration dataset, eval_delay >=1 and/or prefer a larger window size, using panel_split may not be feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example of where panel split is feasible\n",
    "\n",
    "#generate the dataframe\n",
    "window_size = 1\n",
    "cat_method = {'isocode': 'one_hot_encode'}\n",
    "df, features, target_col = lpci.prepare_df(\n",
    "    window_size,\n",
    "    cat_method = cat_method,\n",
    ")\n",
    "\n",
    "#we only want to tune hyperparameters for the quantile regression forest based on data from the calibration set.\n",
    "lpci._dtype_check(df, lpci.time_col, np.dtype('int64')) #check that the time_col is of type int\n",
    "train_df = df[df[lpci.time_col].isin(lpci.unique_cal_time)].sort_values(by = lpci.id_vars).reset_index(drop=True)\n",
    "#note that the above 3 lines are handled inside LPCI.tune() method\n",
    "\n",
    "display(train_df)\n",
    "\n",
    "#initialize the panel split\n",
    "cv_kwargs = {\n",
    "    'n_splits': 2, \n",
    "    'gap': 0, \n",
    "    'test_size': 1, \n",
    "    'progress_bar': False, \n",
    "    'plot': True\n",
    "    }\n",
    "panel_split = PanelSplit(train_df['year'], **cv_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above, given the range of years in our calibration set, the eval_delay and selected window size we end up with 3 unique years in our train_df -> [2017, 2018, 2019]. Perfect, we can use panelsplit and do time-series cross validation during hyperparameter tuning.\n",
    "\n",
    "But now see the below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the dataframe\n",
    "window_size = 2\n",
    "cat_method = {'isocode': 'one_hot_encode'}\n",
    "df, features, target_col = lpci.prepare_df(\n",
    "    window_size,\n",
    "    cat_method = cat_method,\n",
    ")\n",
    "\n",
    "#we only want to tune hyperparameters for the quantile regression forest based on data from the calibration set.\n",
    "lpci._dtype_check(df, lpci.time_col, np.dtype('int64')) #check that the time_col is of type int\n",
    "train_df = df[df[lpci.time_col].isin(lpci.unique_cal_time)].sort_values(by = lpci.id_vars).reset_index(drop=True)\n",
    "#note that the above 3 lines are handled inside LPCI.tune() method\n",
    "\n",
    "display(train_df)\n",
    "\n",
    "#initialize the panel split\n",
    "n_splits = 2\n",
    "panel_split = PanelSplit(train_df['year'], n_splits=n_splits, gap=0, test_size=1, progress_bar=False, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a window size of 2, our train_df is restricted to only [2018, 2019]. This is an insufficient sample to undertake time-series cross-validation.\n",
    "\n",
    "So there is a trade-off here. In this case, we will proceed with standard cross-validation as we prefer a longer window size (i.e. more lagged residuals = more features). This will be specific to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "cat_method = {'isocode': 'one_hot_encode'}\n",
    "df, features, target_col = lpci.prepare_df(\n",
    "    window_size,\n",
    "    cat_method = cat_method,\n",
    ")\n",
    "\n",
    "#we only want to tune hyperparameters for the quantile regression forest based on data from the calibration set.\n",
    "lpci._dtype_check(df, lpci.time_col, np.dtype('int64')) #check that the time_col is of type int\n",
    "train_df = df[df[lpci.time_col].isin(lpci.unique_cal_time)].sort_values(by = lpci.id_vars).reset_index(drop=True)\n",
    "#note that the above 3 lines are handled inside LPCI.tune() method\n",
    "\n",
    "display(train_df)\n",
    "cv = 3\n",
    "\n",
    "#note that if you want to use panel_split for cross-validation, \n",
    "# then you will need a dictionary like the below to pass to the tune method\n",
    "# cv_kwargs = {\n",
    "#     'n_splits': 2, \n",
    "#     'gap': 0, \n",
    "#     'test_size': 1, \n",
    "#     'progress_bar': False, \n",
    "#     'plot': True\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the remaining parameters required for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantiles\n",
    "\n",
    "alpha = 0.1 #our significance level for the prediction interval\n",
    "n_quantiles = 2 #the number of quantiles we want either side of the prediction interval\n",
    "\n",
    "#note that alpha and n_quantiles are used in combination to generate the quantiles used for prediction in the Quantile Random Forest.\n",
    "quantiles = lpci.gen_quantiles(alpha, n_quantiles)\n",
    "print(\"These are the quantiles for which the Quantile Random Forest will generate predictions.\")\n",
    "display(quantiles)\n",
    "del quantiles\n",
    "#note that the generation of quantiles is handled inside the LPCI.tune() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search - you can use GridSearchCV or RandomizedSearchCV\n",
    "\n",
    "# grid_search_method = 'GridSearchCV'\n",
    "# grid_search_kwargs = {\n",
    "#     'param_grid' : {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'max_depth': [5, 10, 15],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4],\n",
    "#         'max_features': ['sqrt', 'log2'],\n",
    "#     },\n",
    "#     'scoring' : None, #careful. None uses sklearn's default scoring metric for the model (regression --> r2_score). \n",
    "#     #If you want to use something else, it will need to handle multi-output regression (since we are predicting quantiles).\n",
    "#     'n_jobs' : -1,\n",
    "# }\n",
    "\n",
    "from scipy.stats import randint\n",
    "grid_search_method = 'RandomizedSearchCV'\n",
    "grid_search_kwargs = {\n",
    "    'param_distributions' : {\n",
    "        'max_depth': randint(2, 10), \n",
    "        'n_estimators': randint(10, 200),\n",
    "        'min_samples_leaf': randint(2, 15),\n",
    "    },\n",
    "    'scoring' : None, #careful. None uses sklearn's default scoring metric for the model (regression --> r2_score). \n",
    "    #If you want to use something else, it will need to handle multi-output regression (since we are predicting quantiles).\n",
    "    'n_jobs' : -1,\n",
    "    'n_iter': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can tune our model. Recall that our objective is to predict residuals in time t using lagged residuals as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Features: {features}\")\n",
    "\n",
    "print(\"Fitting the Quantile Random Forest.\")\n",
    "best_params, quantiles  = lpci.tune(\n",
    "    df = df,\n",
    "    features = features,\n",
    "    target_col = target_col,\n",
    "    alpha = alpha,\n",
    "    n_quantiles = n_quantiles,\n",
    "    grid_search_kwargs=grid_search_kwargs,\n",
    "    grid_search_method=grid_search_method,\n",
    "    cv_kwargs=cv,\n",
    "    return_best_estimator = False   \n",
    ")\n",
    "\n",
    "best_params.update({'random_state': 42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are at crunch time! We are ready to generate prediction intervals. The entire procedure is contained in LPCI.fit_predict(), but let's break it down step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) We generate our quantiles - these should be exactly the same as the ones used in the tuning process.\n",
    "lpci.gen_quantiles(alpha, n_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) initialize our panel split object. This is very important. \n",
    "#In contrast to the tuning process, we definitely want to utilize the panel split object here.\n",
    "#This is because we want to maximize the number of observations for fitting the model.\n",
    "#First, we run a check to guarantee that at least one unique time period from the calibration set is present in the df.\n",
    "#This is necessary because otherwise, the first test set observation will have no observations for fitting.\n",
    "# lpci._train_size_check(df)\n",
    "\n",
    "#initialize our panelsplit object\n",
    "panel_split_kwargs = {\n",
    "    'gap': 0,\n",
    "    'test_size': 1,\n",
    "    'progress_bar': False,\n",
    "    'plot': True\n",
    "}\n",
    "panel_split = PanelSplit(df[lpci.time_col], n_splits= lpci._get_n_splits(df[lpci.time_col].unique(), min(lpci.unique_test_time)), **panel_split_kwargs)\n",
    "\n",
    "#empty dataframe to store the predictions\n",
    "interval_df = panel_split.gen_test_labels(df[id_vars + [target_col]])\n",
    "\n",
    "#merge back on the original predictions and target from the test set\n",
    "interval_df = interval_df.reset_index() #to preserve the index\n",
    "interval_df = interval_df.merge(lpci.test_preds[lpci.id_vars + [lpci.preds_col, lpci.true_col]], on = lpci.id_vars, how = 'left')\n",
    "interval_df = interval_df.set_index('index')\n",
    "interval_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) initialize the model\n",
    "best_params = {'n_estimators': 100}\n",
    "quantiles = lpci.gen_quantiles(alpha, n_quantiles)\n",
    "qrf = RandomForestQuantileRegressor(q = quantiles, **best_params)\n",
    "\n",
    "#and obtain fitted estimators for each split\n",
    "fitted_estimators = panel_split.cross_val_fit(\n",
    "    estimator = qrf,\n",
    "    X = df[features],\n",
    "    y = df[target_col],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) generate the predictions of the residuals. test_preds is a list of tuples [(np.array, pd.Index)]\n",
    "n_jobs = -1\n",
    "\n",
    "if n_jobs <= -1:\n",
    "    num_processes = os.cpu_count()\n",
    "elif n_jobs > 0:\n",
    "    num_processes = n_jobs  # Use the specified number of jobs\n",
    "else:\n",
    "    raise ValueError(f\"Invalid n_jobs value: {n_jobs}\")\n",
    "\n",
    "# Use multiprocessing for parallel processing\n",
    "args = [\n",
    "    (fitted_estimators[i], df.loc[test_indices, features].copy(), i)\n",
    "    for i, (_, test_indices) in enumerate(panel_split.split())\n",
    "]\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    test_preds = pool.map(LPCI.predict_split_mp, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at one of the prediction arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_interval_preds = test_preds[0]\n",
    "\n",
    "print('the first element are the predictions of the residuals (n_samples, n_quantiles)')\n",
    "display(example_interval_preds[0].shape)\n",
    "display(example_interval_preds[0])\n",
    "\n",
    "print('the second element are the indices associated with the predictions')\n",
    "(example_interval_preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the task is to compute the prediction interval. Let's take our example interval preds and see how this works. The method is contained in LPCI.gen_conf_interval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we split the interval preds into the upper and lower quanitiles\n",
    "preds = example_interval_preds[0] #recall that this of shape (n_samples, n_quantiles)\n",
    "mid_quantile = int(len(quantiles)/2)\n",
    "lower_quantile_preds = preds[:, :mid_quantile]\n",
    "upper_quantile_preds = preds[:, mid_quantile:]\n",
    "\n",
    "print(\"The predictions for the lower quantile\")\n",
    "display(lower_quantile_preds)\n",
    "\n",
    "print(\"The predictions for the upper quantile\")\n",
    "display(upper_quantile_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we compute the width of the prediction interval\n",
    "widths = upper_quantile_preds - lower_quantile_preds\n",
    "display(widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the narrowest prediction interval. The index of which is represented by i_stars. i.e. we compute the prediction interval using the pair of quantiles with the smallest width for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.argmin(widths, axis = 1)) \n",
    "i_stars = np.argmin(widths, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using i_stars we extract the prediction for the pair of quantiles with the smallest width for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lower and upper confidence intervals\n",
    "lower_conf = lower_quantile_preds[np.arange(len(i_stars)), i_stars]\n",
    "upper_conf = upper_quantile_preds[np.arange(len(i_stars)), i_stars]\n",
    "\n",
    "#get the optimal quantiles\n",
    "opt_lower_q = quantiles[:mid_quantile][i_stars]\n",
    "opt_upper_q = quantiles[mid_quantile:][i_stars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack the predictions of the intervals of the residuals\n",
    "for (preds, index) in test_preds:\n",
    "    #save the predictions of the residuals for each quantile\n",
    "    interval_df.loc[index, [f'q_{np.round(i, 5)}' for i in quantiles]] = preds\n",
    "\n",
    "    #compute the lower and upper confidence intervals\n",
    "    lower_conf, upper_conf, opt_lower_q, opt_upper_q = lpci.gen_conf_interval(preds, quantiles)\n",
    "    interval_df.loc[index, f'{target_col}_lower_conf'] = lower_conf\n",
    "    interval_df.loc[index, f'{target_col}_upper_conf'] = upper_conf\n",
    "    interval_df.loc[index, 'opt_lower_q'] = opt_lower_q\n",
    "    interval_df.loc[index, 'opt_upper_q'] = opt_upper_q\n",
    "\n",
    "#add the prediction of the residuals back to the point prediction to compute our final prediction interval\n",
    "interval_df['lower_conf'] = interval_df[lpci.preds_col] + interval_df[f'{target_col}_lower_conf']\n",
    "interval_df['upper_conf'] = interval_df[lpci.preds_col] + interval_df[f'{target_col}_upper_conf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interval_df)\n",
    "\n",
    "cols_to_keep = ['isocode', 'year', 'residuals', 'preds', 'true', 'lower_conf', 'upper_conf', 'residuals_lower_conf', 'residuals_upper_conf']\n",
    "display(interval_df[cols_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you seen the complete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete function\n",
    "interval_df = lpci.fit_predict(\n",
    "    df = df,\n",
    "    features = features,\n",
    "    target_col = target_col,\n",
    "    best_params = best_params,\n",
    "    alpha = alpha,\n",
    "    n_quantiles = n_quantiles,\n",
    "    panel_split_kwargs=panel_split_kwargs,\n",
    "    n_jobs = n_jobs,\n",
    "    return_fitted_estimators=False #if you want to return the fitted estimators\n",
    ")\n",
    "\n",
    "display(interval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the evaluator. \n",
    "#note that the lpci instance is necessary to retain a number of key attributes e.g. unit_col, time_col etc.\n",
    "evaluator = EvaluateLPCI(\n",
    "    lpci,\n",
    "    alpha,\n",
    "    interval_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Coverage metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage metrics for different subsets of the data can be computed. \n",
    "Recall that coverage is defined as the share of times the target variable falls within the prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall coverage of the prediction intervals.\")\n",
    "display(evaluator.overall_coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coverage by year\")\n",
    "display(evaluator.coverage_by_time())\n",
    "print(\"Coverage by unit\")\n",
    "display(evaluator.coverage_by_unit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coverage by bin\")\n",
    "bins = [0, 1, 100, 1000, 10000, 100000, float('inf')]\n",
    "labels = ['0', '1-100', '100-1000', '1000-10000', '10000-100000', '100000+']\n",
    "\n",
    "display(evaluator.coverage_by_bin(bins, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can plot the prediction intervals for a given year for all units\n",
    "fig = evaluator.plot_intervals_year(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also plot the prediction intervals for a given unit for all years\n",
    "fig = evaluator.plot_intervals_unit('DEU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
